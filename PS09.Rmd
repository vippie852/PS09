---
title: "STAT/MATH 495: Problem Set 09"
author: "Vickie Ip"
date: "2017-11-07"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

library(tidyverse)
library(cowplot)
```



# Collaboration

Please indicate who you collaborated with on this assignment: I got help from Andrew Kim


# Question 1: Run k-means

```{r}
observations_1 <- read_csv("data/observations_1.csv")
observations_2 <- read_csv("data/observations_2.csv")

# Set observations to be one of two datasets
observations <- observations_1
```

# Observation 1 Results

```{r}
observations <- observations_1
cluster_center_list <- list()
mutated_observation_list <- list()
set.seed(1002)
for (i in 1:10){
  k <- 2
  k_means_results <- kmeans(observations, centers = k)
  clusters <- k_means_results$cluster
  cluster_centers <- k_means_results$centers
  
  # Add cluster results to observations. Note we convert to factor since cluster
  # ID's should be treated as categorical
  dataframe <- data.frame(x1 = observations[,1], x2 = observations[,2], cluster = as.factor(clusters))
  mutated_observation_list[[i]] <- dataframe

  # Add cluster ID's to cluster_centers
  cluster_centers <- cluster_centers %>% 
    as_tibble() %>% 
    mutate(cluster=as.factor(1:k))
  cluster_center_list[[i]] <- cluster_centers
}

plots <- list()

for (i in 1:10){
  plot <- ggplot(NULL, aes(x=x1, y=x2, col=cluster)) +
  geom_point(data=mutated_observation_list[[i]]) +
  geom_point(data=cluster_center_list[[i]], size=5)
  
  plots[[i]] <- plot
}

cowplot::plot_grid(plotlist = plots, nrow = 5, ncol = 2)
```

**Questions**:

1. Run KMC 10 times on `observations_1` and comment on the consistency of the
results.
2. Speculate on the root cause of any consistency or inconsistency in the
results.

# Observation 2 Results

```{r}
observations <- observations_1
cluster_center_list <- list()
mutated_observation_list <- list()
set.seed(1002)
for (i in 1:10){
  k <- 2
  k_means_results <- kmeans(observations, centers = k)
  clusters <- k_means_results$cluster
  cluster_centers <- k_means_results$centers
  
  # Add cluster results to observations. Note we convert to factor since cluster
  # ID's should be treated as categorical
  dataframe <- data.frame(x1 = observations[,1], x2 = observations[,2], cluster = as.factor(clusters))
  mutated_observation_list[[i]] <- dataframe

  # Add cluster ID's to cluster_centers
  cluster_centers <- cluster_centers %>% 
    as_tibble() %>% 
    mutate(cluster=as.factor(1:k))
  cluster_center_list[[i]] <- cluster_centers
}

plots <- list()

for (i in 1:10){
  plot <- ggplot(NULL, aes(x=x1, y=x2, col=cluster)) +
  geom_point(data=mutated_observation_list[[i]]) +
  geom_point(data=cluster_center_list[[i]], size=5)
  
  plots[[i]] <- plot
}

cowplot::plot_grid(plotlist = plots, nrow = 5, ncol = 2)
```

**Questions**:

3. Run KMC 10 times on `observations_2` and comment on the consistency of the
results.
4. Speculate on the root cause of any consistency or inconsistency in the
results.

**Answers**:

1. After running KMC 10 times, I noticed that in 5 out of the 10 times, the blue cluster was predicted to have a `x2` value of lower than 1.0 and vice versa. This shows that there's equal chance that the clusters can be predicted to have a `x2` value above or below 1.0. 

2. This low consistency is because the points are quite close together and there is no clear distinction between one cluster to the other. The initialization of the centroid plays a more important part in determining the end result.

3. After running KMC 10 times on `observations_2`, I noticed that the predictive consistency is much higher for these observations as compared with `observations_1`. 8 out of the 10 times showed that the red cluster has a `x2` value of less than 1.0 which indicates strong predictive accuracy.

4. The consistency is much higher for `observations_2` because there is a clearer distinction to which cluster the points belong to. The initialization of the centroid plays a less significant part in the end result because of how far apart the points are. 

# Bonus question: Code your own

Read ISLR page 388 Algorithm 10.1 and implement k-means clustering from scratch.
Don't worry about doing it for general $k$; keep it simple and do it for $k=2$
specifically. Apply it to `observations_2` from above.

```{r}
# Hint:
library(proxy)
A <- data_frame(
  x1 = c(0, 0.5, 0.75, 1),
  x2 = c(0, 0.5, 0.75, 1)
)
B <- data_frame(
  x1 = c(1, 0),
  x2 = c(1, 0)
)
distance_matrix <- proxy::dist(x=A, y=B)
distance_matrix
apply(distance_matrix, 1, which.min)

clusters <- rep(c(1,2), 100)
obs2 <- observations_2
tgt <- cbind(obs2, clusters)

#calculate avg x and y coord for clusters 1 and 2 ) - this is in a for loop 100 times
#save averages
#for each point, calculate distance between each avg
```


```{r}
obs2 <- observations_2
x1 <- observations_2$x1
x2 <- observations_2$x2

Spec = c(rep(1,50), rep(2,50), rep(3,50))

#Randomly pick 2 centroids
set.seed(200)
x1_pt <- runif(2,range(x1)[1], range(x1)[2])
set.seed(400)
x2_pt <- runif(2, range(x2) [2], range(x2) [2])

centroid = matrix(1:4, nrow=2)

for( i in 1:2) { centroid[i,] = c(x2_pt[i], x1_pt[i])}

#Calculate Distance
calc_dist <- function(x,y) {return(sum((x-y) ^2))}

distances <- rep(NA,2)
assignments <- rep(NA, 100) -> assignments_new
for (i in 1:100) {
  for (j in 1:2) {
    distances[j] = calc_dist(observations_2[i,],rev(centroid[j,]))
  }
  assignments[i] <- which.min(distances)
}
```
```{r}
#Re-calculate centroids and reassign clusters
for(i in 1:2) {
  centroid[i,1] <- mean(x2[which(assignments == i)])
  centroid[i,2] <- mean(x1[which(assignments == i)])
}

for (i in 1:100) {
  for (j in 1:2) {
    distances[j] = calc_dist(observations_2[i,],rev(centroid[j,]))
  }
  assignments_new[i] <- which.min(distances)
}

plot(x1~x2, pch=16, ylab="x2", xlab="x1", col=assignments_new,
     main="Clustering after final iteration")
for(i in 1:2){points(centroid[i,1],centroid[i,2] , col=i, pch=17, cex=2)}
for(i in 1:100){points(y=x1[i],x=x2[i],pch=4, col=Spec[i])}
legend("topleft", c("Original Cluster", "K-Means assignment"),pch=c(4,16),bty="n")
```


